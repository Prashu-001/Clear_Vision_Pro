{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893116cc-80c1-4740-9c27-74a18441ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from srwgan_model import Generator, Discriminator\n",
    "from losses import build_vgg, perceptual_loss, wasserstein_loss, mse_loss\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create samples directory\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "generator = Generator(12)\n",
    "discriminator = Discriminator()\n",
    "vgg_model = build_vgg()\n",
    "dummy_input = tf.random.normal((1, 128, 128, 3))\n",
    "vgg_model(dummy_input) \n",
    "g_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "d_optimizer = tf.keras.optimizers.RMSprop(5e-5)\n",
    "#Dummy forward pass (ensure variable creation)\n",
    "generator(tf.zeros((1, 64, 64, 3)))\n",
    "discriminator(tf.zeros((1, 128, 128, 3)))\n",
    "\n",
    "# === Training Step Function ===\n",
    "@tf.function\n",
    "def train_step(lr_batch, hr_batch, critic_iter):\n",
    "    batch_size = tf.shape(lr_batch)[0]\n",
    "    real_labels = -tf.ones((batch_size, 1))\n",
    "    fake_labels = tf.ones((batch_size, 1))\n",
    "\n",
    "    # === Train Discriminator multiple times ===\n",
    "    for _ in range(critic_iter):\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            super_res = generator(lr_batch, training=True)  # Forward pass only\n",
    "            real_output = discriminator(hr_batch, training=True)\n",
    "            fake_output = discriminator(super_res, training=True)\n",
    "\n",
    "            d_loss_real = wasserstein_loss(real_labels, real_output)\n",
    "            d_loss_fake = wasserstein_loss(fake_labels, fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        disc_grads = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "        # === Weight Clipping ===\n",
    "        for var in discriminator.trainable_variables:\n",
    "            var.assign(tf.clip_by_value(var, -0.05, 0.05))\n",
    "\n",
    "    # === Train Generator once ===\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        super_res = generator(lr_batch, training=True)\n",
    "        fake_output = discriminator(super_res, training=True)\n",
    "\n",
    "        adv_loss = wasserstein_loss(real_labels, fake_output)\n",
    "        cont_loss = mse_loss(hr_batch, super_res)\n",
    "        perc_loss = perceptual_loss(vgg_model, hr_batch, super_res)\n",
    "\n",
    "        g_loss = 1e-3 * adv_loss + 0.01 * perc_loss + cont_loss\n",
    "\n",
    "    gen_grads = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
    "\n",
    "    return g_loss, d_loss\n",
    "\n",
    "# === Sample Image Saving ===\n",
    "def save_sample_output(epoch, generator, val_dataset):\n",
    "    for lr_img, hr_img in val_dataset.take(1):\n",
    "        sr_img = generator(lr_img, training=False)\n",
    "        sr = array_to_img(sr_img[5])\n",
    "        hr = array_to_img(hr_img[5])\n",
    "        lr = array_to_img(tf.image.resize(lr_img[5], (128, 128)))  # for visual comparison\n",
    "\n",
    "        stacked = np.hstack([np.array(lr), np.array(sr), np.array(hr)])\n",
    "        plt.imsave(f\"samples/epoch_{epoch:03}.png\", stacked.astype(\"uint8\"))\n",
    "        break\n",
    "\n",
    "def train(train_dataset, val_dataset, epochs, patience=10, start_epoch=0, critic_iter=5):\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        g_loss_metric = tf.keras.metrics.Mean()\n",
    "        d_loss_metric = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for step, (lr_batch, hr_batch) in enumerate(train_dataset, start=1):\n",
    "            g_loss, d_loss = train_step(lr_batch, hr_batch, critic_iter)\n",
    "            g_loss_metric.update_state(g_loss)\n",
    "            d_loss_metric.update_state(d_loss)\n",
    "\n",
    "        avg_gen_loss = g_loss_metric.result()\n",
    "        avg_disc_loss = d_loss_metric.result()\n",
    "        print(f\"Epoch {epoch + 1} Summary â†’ Gen Loss: {avg_gen_loss:.4f} Disc Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "        # === Save samples and checkpoints ===\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            save_sample_output(epoch + 1, generator, val_dataset)\n",
    "            manager.save(checkpoint_number=epoch + 1)\n",
    "            generator.save(f\"/kaggle/working/checkpoints/srw_generator_epoch_{epoch+1}.h5\")\n",
    "            discriminator.save(f\"/kaggle/working/checkpoints/srw_discriminator_epoch_{epoch+1}.h5\")\n",
    "\n",
    "# Checkpointing\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                            discriminator=discriminator,\n",
    "                            generator_optimizer=g_optimizer,\n",
    "                            discriminator_optimizer=d_optimizer)\n",
    "manager = tf.train.CheckpointManager(ckpt, './srgan_ckpts', max_to_keep=5)\n",
    "\n",
    "# Restore if checkpoint exists\n",
    "if manager.latest_checkpoint:\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    print(f\"âœ… Restored from checkpoint: {manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"ðŸ†• Training from scratch\")\n",
    "\n",
    "# === Resume Support ===\n",
    "def get_latest_epoch(manager):\n",
    "    if manager.latest_checkpoint:\n",
    "        ckpt_name = os.path.basename(manager.latest_checkpoint)\n",
    "        try:\n",
    "            return int(ckpt_name.split('-')[-1])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "latest_epoch = get_latest_epoch(manager)\n",
    "# === Start Training ===\n",
    "train(train_dataset, val_dataset, epochs=50, patience=10, start_epoch=latest_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
