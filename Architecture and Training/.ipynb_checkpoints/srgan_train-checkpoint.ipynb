{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e85a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "\n",
    "from srgan_model import Generator, Discriminator\n",
    "from losses import build_vgg, bce_loss\n",
    "\n",
    "\n",
    "# === Loss Functions ===\n",
    "bce = bce_loss(from_logits=True)\n",
    "vgg = build_vgg()\n",
    "\n",
    "def generator_loss(high_res, super_res, fake_output):\n",
    "    content_loss = tf.reduce_mean(tf.square(vgg(high_res) - vgg(super_res)))\n",
    "    adversarial_loss = bce(tf.ones_like(fake_output), fake_output)\n",
    "    return content_loss + (1e-3 * adversarial_loss)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = bce(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = bce(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "\n",
    "# === Setup ===\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "generator = Generator(10)\n",
    "discriminator = Discriminator()\n",
    "\n",
    "gen_optimizer = keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9)\n",
    "disc_optimizer = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9)\n",
    "\n",
    "# Dummy forward pass to initialize variables\n",
    "generator(tf.zeros((1, 64, 64, 3)))\n",
    "discriminator(tf.zeros((1, 128, 128, 3)))\n",
    "\n",
    "\n",
    "# === Training Step Function ===\n",
    "@tf.function\n",
    "def train_step(low_res, high_res, disc_steps=5):\n",
    "    for _ in range(disc_steps):\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            super_res = generator(low_res, training=True)\n",
    "            real_output = discriminator(high_res, training=True)\n",
    "            fake_output = discriminator(super_res, training=True)\n",
    "            disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        grads_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "        disc_optimizer.apply_gradients(zip(grads_disc, discriminator.trainable_variables))\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        super_res = generator(low_res, training=True)\n",
    "        fake_output = discriminator(super_res, training=True)\n",
    "        gen_loss = generator_loss(high_res, super_res, fake_output)\n",
    "\n",
    "    grads_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(grads_gen, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "\n",
    "# === Checkpointing ===\n",
    "ckpt = tf.train.Checkpoint(generator=generator,\n",
    "                           discriminator=discriminator,\n",
    "                           generator_optimizer=gen_optimizer,\n",
    "                           discriminator_optimizer=disc_optimizer)\n",
    "manager = tf.train.CheckpointManager(ckpt, './srgan_ckpts', max_to_keep=5)\n",
    "\n",
    "if manager.latest_checkpoint:\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    print(f\"âœ… Restored from checkpoint: {manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"ðŸ†• Training from scratch\")\n",
    "\n",
    "\n",
    "# === Save Sample Output ===\n",
    "def save_sample_output(epoch, generator, val_dataset):\n",
    "    for lr_img, hr_img in val_dataset.take(1):\n",
    "        sr_img = generator(lr_img, training=False)\n",
    "        sr = array_to_img(sr_img[0])\n",
    "        hr = array_to_img(hr_img[0])\n",
    "        lr = array_to_img(tf.image.resize(lr_img[0], (128, 128)))  # upscale LR for comparison\n",
    "\n",
    "        stacked = np.hstack([np.array(lr), np.array(sr), np.array(hr)])\n",
    "        plt.imsave(f\"samples/epoch_{epoch:03}.png\", stacked.astype(\"uint8\"))\n",
    "        break\n",
    "\n",
    "\n",
    "# === Resume Helper ===\n",
    "def get_latest_epoch(manager):\n",
    "    if manager.latest_checkpoint:\n",
    "        ckpt_name = os.path.basename(manager.latest_checkpoint)\n",
    "        try:\n",
    "            return int(ckpt_name.split('-')[-1])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "# === Training Loop ===\n",
    "def train(train_dataset, val_dataset, epochs, patience=10, start_epoch=0):\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        g_loss_metric = tf.keras.metrics.Mean()\n",
    "        d_loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "        for step, (lr_batch, hr_batch) in enumerate(train_dataset, start=1):\n",
    "            gen_loss, disc_loss = train_step(lr_batch, hr_batch)\n",
    "            g_loss_metric.update_state(gen_loss)\n",
    "            d_loss_metric.update_state(disc_loss)\n",
    "\n",
    "        avg_gen_loss = g_loss_metric.result()\n",
    "        avg_disc_loss = d_loss_metric.result()\n",
    "        print(f\"Epoch {epoch + 1} Summary â†’ Gen Loss: {avg_gen_loss:.4f}, Disc Loss: {avg_disc_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            save_sample_output(epoch + 1, generator, val_dataset)\n",
    "            manager.save(checkpoint_number=epoch + 1)\n",
    "            generator.save(f\"checkpoints/generator_epoch_{epoch + 1}.h5\")\n",
    "            discriminator.save(f\"checkpoints/discriminator_epoch_{epoch + 1}.h5\")\n",
    "\n",
    "\n",
    "# === Start Training ===\n",
    "latest_epoch = get_latest_epoch(manager)\n",
    "\n",
    "# Define your datasets before running the training:\n",
    "# train_dataset = ...\n",
    "# val_dataset = ...\n",
    "# train(train_dataset, val_dataset, epochs=50, start_epoch=latest_epoch)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
